{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# distributed training:\n- is a model paradigm where training workload is spread acress multiple worker nodes.\n- used for large models.\n### Ways to perform distributed training:\n- DistributedDataParallel\n- Fully Sharded Data Parallel\n- Tensor Parallel\n- Device Mesh\n- Remote Procedure Call distributed training\n- custom Extensions\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import os\n\n# The distribution API is only implemented for the JAX backend for now.\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\nimport keras\nfrom keras import layers\nimport jax\nimport numpy as np\nfrom tensorflow import data as tf_data  # For dataset input.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:46:08.653096Z","iopub.execute_input":"2025-06-22T10:46:08.653393Z","iopub.status.idle":"2025-06-22T10:46:26.481159Z","shell.execute_reply.started":"2025-06-22T10:46:08.653363Z","shell.execute_reply":"2025-06-22T10:46:26.480104Z"}},"outputs":[{"name":"stderr","text":"2025-06-22 10:46:10.547143: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750589170.761449      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750589170.823454      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"devices","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:06:26.266747Z","iopub.execute_input":"2025-06-22T10:06:26.267406Z","iopub.status.idle":"2025-06-22T10:06:26.272532Z","shell.execute_reply.started":"2025-06-22T10:06:26.267381Z","shell.execute_reply":"2025-06-22T10:06:26.271706Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[CudaDevice(id=0), CudaDevice(id=1)]"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Retrieve the local available gpu devices.\ndevices = jax.devices(\"gpu\")  # Assume it has 8 local GPUs.\n\n# Define a 2x4 device mesh with data and model parallel axes\nmesh = keras.distribution.DeviceMesh(\n    shape=(2,1), axis_names=[\"data\", \"model\"], devices=devices\n)\n\n# A 2D layout, which describes how a tensor is distributed across the\n# mesh. The layout can be visualized as a 2D grid with \"model\" as rows and\n# \"data\" as columns, and it is a [4, 2] grid when it mapped to the physical\n# devices on the mesh.\nlayout_2d = keras.distribution.TensorLayout(axes=(\"model\", \"data\"), device_mesh=mesh)\n\n# A 4D layout which could be used for data parallel of a image input.\nreplicated_layout_4d = keras.distribution.TensorLayout(\n    axes=(\"data\", None, None, None), device_mesh=mesh\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:09:15.869216Z","iopub.execute_input":"2025-06-22T10:09:15.869486Z","iopub.status.idle":"2025-06-22T10:09:15.874363Z","shell.execute_reply.started":"2025-06-22T10:09:15.869466Z","shell.execute_reply":"2025-06-22T10:09:15.873557Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"layout_2d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:07:04.165776Z","iopub.execute_input":"2025-06-22T10:07:04.166093Z","iopub.status.idle":"2025-06-22T10:07:04.171390Z","shell.execute_reply.started":"2025-06-22T10:07:04.166070Z","shell.execute_reply":"2025-06-22T10:07:04.170793Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<TensorLayout axes=('model', 'data'), device_mesh=<DeviceMesh shape=(2, 1), axis_names=['data', 'model']>>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"replicated_layout_4d","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:09:51.598128Z","iopub.execute_input":"2025-06-22T10:09:51.598599Z","iopub.status.idle":"2025-06-22T10:09:51.603222Z","shell.execute_reply.started":"2025-06-22T10:09:51.598578Z","shell.execute_reply":"2025-06-22T10:09:51.602698Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<TensorLayout axes=('data', None, None, None), device_mesh=<DeviceMesh shape=(2, 1), axis_names=['data', 'model']>>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Data Parallel\n- model weights are replicated across all devices in deviceMesh and each device processes a  portion of input data","metadata":{}},{"cell_type":"code","source":"def b():\n    # Create DataParallel with list of devices.\n    # As a shortcut, the devices can be skipped,\n    # and Keras will detect all local available devices.\n    # E.g. data_parallel = DataParallel()\n    data_parallel = keras.distribution.DataParallel(devices=devices)\n\n    # Or you can choose to create DataParallel with a 1D `DeviceMesh`.\n    mesh_1d = keras.distribution.DeviceMesh(\n        shape=(2,), axis_names=[\"data\"], devices=devices\n    )\n    data_parallel = keras.distribution.DataParallel(device_mesh=mesh_1d)\n\n    inputs = np.random.normal(size=(128, 28, 28, 1))\n    labels = np.random.normal(size=(128, 10))\n    dataset = tf_data.Dataset.from_tensor_slices((inputs, labels)).batch(16)\n\n    # Set the global distribution.\n    keras.distribution.set_distribution(data_parallel)\n\n    # Note that all the model weights from here on are replicated to\n    # all the devices of the `DeviceMesh`. This includes the RNG\n    # state, optimizer states, metrics, etc. The dataset fed into `model.fit` or\n    # `model.evaluate` will be split evenly on the batch dimension, and sent to\n    # all the devices. You don't have to do any manual aggregration of losses,\n    # since all the computation happens in a global context.\n    inputs = layers.Input(shape=(28, 28, 1))\n    y = layers.Flatten()(inputs)\n    y = layers.Dense(units=200, use_bias=False, activation=\"relu\")(y)\n    y = layers.Dropout(0.4)(y)\n    y = layers.Dense(units=10, activation=\"softmax\")(y)\n    model = keras.Model(inputs=inputs, outputs=y)\n\n    model.compile(loss=\"mse\")\n    model.fit(dataset, epochs=3)\n    model.evaluate(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:35:47.876176Z","iopub.execute_input":"2025-06-22T10:35:47.876435Z","iopub.status.idle":"2025-06-22T10:35:47.884133Z","shell.execute_reply.started":"2025-06-22T10:35:47.876417Z","shell.execute_reply":"2025-06-22T10:35:47.883388Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def a():\n    inputs = np.random.normal(size=(128, 28, 28, 1))\n    labels = np.random.normal(size=(128, 10))\n    dataset = tf_data.Dataset.from_tensor_slices((inputs, labels)).batch(16)\n\n    inputs = layers.Input(shape=(28, 28, 1))\n    y = layers.Flatten()(inputs)\n    y = layers.Dense(units=200, use_bias=False, activation=\"relu\")(y)\n    y = layers.Dropout(0.4)(y)\n    y = layers.Dense(units=10, activation=\"softmax\")(y)\n    model = keras.Model(inputs=inputs, outputs=y)\n\n    model.compile(loss=\"mse\")\n    model.fit(dataset, epochs=3)\n    model.evaluate(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:46:31.082520Z","iopub.execute_input":"2025-06-22T10:46:31.082824Z","iopub.status.idle":"2025-06-22T10:46:31.089785Z","shell.execute_reply.started":"2025-06-22T10:46:31.082797Z","shell.execute_reply":"2025-06-22T10:46:31.088711Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%time a()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:46:43.910626Z","iopub.execute_input":"2025-06-22T10:46:43.910977Z","iopub.status.idle":"2025-06-22T10:46:44.981775Z","shell.execute_reply.started":"2025-06-22T10:46:43.910951Z","shell.execute_reply":"2025-06-22T10:46:44.981041Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.9947  \nEpoch 2/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9111\nEpoch 3/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8611\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8228  \nCPU times: user 1.02 s, sys: 132 ms, total: 1.16 s\nWall time: 1.07 s\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%time b()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:36:55.041284Z","iopub.execute_input":"2025-06-22T10:36:55.041547Z","iopub.status.idle":"2025-06-22T10:36:56.115700Z","shell.execute_reply.started":"2025-06-22T10:36:55.041530Z","shell.execute_reply":"2025-06-22T10:36:56.114850Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.9529\nEpoch 2/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8703\nEpoch 3/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8244\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7696  \nCPU times: user 1.09 s, sys: 84 ms, total: 1.18 s\nWall time: 1.07 s\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from datetime import datetime\nstart_time = datetime.now()\na()\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:46:51.561635Z","iopub.execute_input":"2025-06-22T10:46:51.562002Z","iopub.status.idle":"2025-06-22T10:46:52.566834Z","shell.execute_reply.started":"2025-06-22T10:46:51.561975Z","shell.execute_reply":"2025-06-22T10:46:52.565923Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 1.1406  \nEpoch 2/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0297\nEpoch 3/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9958\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9517  \nDuration: 0:00:01.000590\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datetime import datetime\nstart_time = datetime.now()\nb()\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:45:26.059487Z","iopub.execute_input":"2025-06-22T10:45:26.060071Z","iopub.status.idle":"2025-06-22T10:45:27.167581Z","shell.execute_reply.started":"2025-06-22T10:45:26.060044Z","shell.execute_reply":"2025-06-22T10:45:27.166731Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 1.0709\nEpoch 2/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9911\nEpoch 3/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9338\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9021  \nDuration: 0:00:01.103680\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import numpy as np\nfrom scipy import stats\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:32:35.708042Z","iopub.execute_input":"2025-06-22T10:32:35.708366Z","iopub.status.idle":"2025-06-22T10:32:35.713119Z","shell.execute_reply.started":"2025-06-22T10:32:35.708345Z","shell.execute_reply":"2025-06-22T10:32:35.712174Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Parallel and LayoutMap\n- MP: split model weights acrosss all devices  useful when model weights are too large.\n\n","metadata":{}},{"cell_type":"code","source":"mesh_2d = keras.distribution.DeviceMesh(\n    shape=(2, 1), axis_names=[\"data\", \"model\"], devices=devices\n)\nlayout_map = keras.distribution.LayoutMap(mesh_2d)\n# The rule below means that for any weights that match with d1/kernel, it\n# will be sharded with model dimensions (4 devices), same for the d1/bias.\n# All other weights will be fully replicated.\nlayout_map[\"d1/kernel\"] = (None, \"model\")\nlayout_map[\"d1/bias\"] = (\"model\",)\n\n# You can also set the layout for the layer output like\nlayout_map[\"d2/output\"] = (\"data\", None)\n\nmodel_parallel = keras.distribution.ModelParallel(layout_map=layout_map, batch_dim_name=\"data\")\n\nkeras.distribution.set_distribution(model_parallel)\n\ninputs = layers.Input(shape=(28, 28, 1))\ny = layers.Flatten()(inputs)\ny = layers.Dense(units=200, use_bias=False, activation=\"relu\", name=\"d1\")(y)\ny = layers.Dropout(0.4)(y)\ny = layers.Dense(units=10, activation=\"softmax\", name=\"d2\")(y)\nmodel = keras.Model(inputs=inputs, outputs=y)\n\n# The data will be sharded across the \"data\" dimension of the method, which\n# has 2 devices.\nmodel.compile(loss=\"mse\")\nmodel.fit(dataset, epochs=3)\nmodel.evaluate(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:16:26.557325Z","iopub.execute_input":"2025-06-22T10:16:26.557639Z","iopub.status.idle":"2025-06-22T10:16:27.957252Z","shell.execute_reply.started":"2025-06-22T10:16:26.557615Z","shell.execute_reply":"2025-06-22T10:16:27.956516Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 1.0292\nEpoch 2/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9319\nEpoch 3/3\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9104\n\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8629  \n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.872115433216095"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"keras.distribution.ModelParallel?","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-22T10:15:55.801920Z","iopub.execute_input":"2025-06-22T10:15:55.802297Z","iopub.status.idle":"2025-06-22T10:15:55.847424Z","shell.execute_reply.started":"2025-06-22T10:15:55.802274Z","shell.execute_reply":"2025-06-22T10:15:55.846548Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlayout_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbatch_dim_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nDistribution that shards model variables.\n\nCompare to `DataParallel` which replicates the variables across all devices,\n`ModelParallel` allows you to shard variables in addition to the input data.\n\nTo construct a `ModelParallel` distribution, you need to provide a\n`DeviceMesh` and a `LayoutMap`.\n\n1. `DeviceMesh` contains physical device information. The axis names in\n    the mesh will be used to map the variable and data layout.\n2. `LayoutMap` contains the mapping between variable paths to their\n    corresponding `TensorLayout`.\n\nExample:\n\n```python\ndevices = list_devices()    # Assume there are 8 devices.\n\n# Create a mesh with 2 devices for data parallelism and 4 devices for\n# model parallelism.\ndevice_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n                         devices=devices)\n# Create a layout map that shard the `Dense` layer and `Conv2D`\n# layer variables on the last dimension.\n# Based on the `device_mesh`, this means the variables\n# will be split across 4 devices. Any other variable that doesn't\n# match any key in the layout map will be fully replicated.\nlayout_map = LayoutMap(device_mesh)\nlayout_map['dense.*kernel'] = (None, 'model')\nlayout_map['dense.*bias'] = ('model',)\nlayout_map['conv2d.*kernel'] = (None, None, None, 'model')\nlayout_map['conv2d.*bias'] = ('model',)\n\ndistribution = ModelParallel(\n    layout_map=layout_map,\n    batch_dim_name='batch',\n)\n\n# Set the global distribution, or via `with distribution.scope():`\nset_distribution(distribution)\n\nmodel = model_creation()\nmodel.compile()\nmodel.fit(data)\n```\n\nYou can quickly update the device mesh shape to change the sharding factor\nof the variables. E.g.\n\n```python\n# With only the shape change for the device mesh, the variables will be\n# sharded across 8 devices instead of 4, which further reduces the memory\n# footprint of variables on each of the device.\ndevice_mesh = DeviceMesh(\n    shape=(1, 8),\n    axis_names=('batch', 'model'),\n    devices=devices,\n)\n```\n\nTo figure out a proper layout mapping rule for all the model variables, you\ncan first list out all the model variable paths, which will be used as the\nkey to map the variables to `TensorLayout`.\n\ne.g.\n\n```python\nmodel = create_model()\nfor v in model.variables:\n    print(v.path)\n```\n\nArgs:\n    layout_map: `LayoutMap` instance which map the variable path to the\n        corresponding tensor layout.\n    batch_dim_name: Optional string, the axis name in the device mesh\n        (of the `layout_map` object)\n        that will be used to distribute data. If unspecified, the\n        first axis from the device mesh will be used.\n\u001b[0;31mFile:\u001b[0m           /usr/local/lib/python3.11/dist-packages/keras/src/distribution/distribution_lib.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# different mesh shapes","metadata":{}},{"cell_type":"code","source":"'full_data_parallel_mesh = keras.distribution.DeviceMesh(\n    shape=(8, 1), axis_names=[\"data\", \"model\"], devices=devices\n)\nmore_data_parallel_mesh = keras.distribution.DeviceMesh(\n    shape=(4, 2), axis_names=[\"data\", \"model\"], devices=devices\n)\nmore_model_parallel_mesh = keras.distribution.DeviceMesh(\n    shape=(2, 4), axis_names=[\"data\", \"model\"], devices=devices\n)\nfull_model_parallel_mesh = keras.distribution.DeviceMesh(\n    shape=(1, 8), axis_names=[\"data\", \"model\"], devices=devices\n)'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}